{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b787bd-aca6-4290-a3d9-d780cd9d2020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2us/step \n",
      "Epoch 1/2\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 80ms/step - accuracy: 0.6227 - loss: 0.6187 - val_accuracy: 0.7771 - val_loss: 0.4737\n",
      "Epoch 2/2\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.8596 - loss: 0.3358 - val_accuracy: 0.7074 - val_loss: 0.5675\n",
      "Epoch 1/2\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 174ms/step - accuracy: 0.7024 - loss: 0.5307 - val_accuracy: 0.8569 - val_loss: 0.3344\n",
      "Epoch 2/2\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 171ms/step - accuracy: 0.8971 - loss: 0.2611 - val_accuracy: 0.8742 - val_loss: 0.3033\n",
      "Epoch 1/2\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 175ms/step - accuracy: 0.7224 - loss: 0.5264 - val_accuracy: 0.8519 - val_loss: 0.3444\n",
      "Epoch 2/2\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 190ms/step - accuracy: 0.8942 - loss: 0.2706 - val_accuracy: 0.8617 - val_loss: 0.3156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cf85f2a1a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Problem 1] Execution of various methods\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\n",
    "\n",
    "max_features = 10000 \n",
    "maxlen = 200        \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "def build_model(rnn_layer):\n",
    "    model = Sequential([\n",
    "        Embedding(max_features, 128),\n",
    "        rnn_layer,\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model_rnn = build_model(SimpleRNN(64))\n",
    "model_rnn.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "model_gru = build_model(GRU(64))\n",
    "model_gru.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "model_lstm = build_model(LSTM(64))\n",
    "model_lstm.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8c3328-528d-4aa1-a0a3-892ad7006c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Dense\n",
    "\n",
    "model_conv = Sequential([\n",
    "    ConvLSTM2D(filters=32, kernel_size=(3, 3),\n",
    "               input_shape=(10, 64, 64, 1), # (time, height, width, channels)\n",
    "               padding=\"same\", return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    ConvLSTM2D(filters=32, kernel_size=(3, 3),\n",
    "               padding=\"same\", return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f73a70b-229e-4ac3-95de-8bb385b4b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1us/step \n",
      "Epoch 1/3\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 123ms/step - accuracy: 0.3887 - loss: 2.7486 - val_accuracy: 0.5178 - val_loss: 1.8184\n",
      "Epoch 2/3\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 125ms/step - accuracy: 0.5198 - loss: 1.8254 - val_accuracy: 0.5574 - val_loss: 1.7602\n",
      "Epoch 3/3\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 129ms/step - accuracy: 0.5518 - loss: 1.7108 - val_accuracy: 0.5686 - val_loss: 1.6950\n"
     ]
    }
   ],
   "source": [
    "#[Problem 2] Comparison on another dataset\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_words = 10000\n",
    "maxlen = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 128),\n",
    "    LSTM(64),\n",
    "    Dense(46, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c9d4e-edc5-49b8-b929-5b4f5f391ce7",
   "metadata": {},
   "source": [
    "**[Problem 3] Other related classes in Keras**\n",
    "\n",
    "The class called RNN in Keras is a kind of general framework for recurrent networks. It does not specify whether the network is SimpleRNN, GRU, or LSTM. Instead, it is like a container that can run any type of recurrent “cell” step by step across a sequence. This is mostly used when you want to build a custom recurrent network from lower-level parts.\n",
    "\n",
    "A SimpleRNNCell represents just one step of a SimpleRNN. When you use the full SimpleRNN layer, it is actually repeating this cell many times, once for each time step in the input sequence. You normally don’t use the cell directly, but it exists in case you want very fine control over what happens inside the network.\n",
    "\n",
    "A GRUCell is the same idea, but for the GRU architecture. It represents a single GRU unit with its reset and update gates. A GRU layer is simply many of these GRUCells applied one after another through the sequence.\n",
    "\n",
    "An LSTMCell is again similar, but for the LSTM. It represents one LSTM unit with its input, forget, and output gates, as well as its internal memory state. The LSTM layer is nothing more than a series of LSTMCells being applied step by step.\n",
    "\n",
    "There is also StackedRNNCells, which lets you combine several different cells together so they act as one larger recurrent layer. For example, you could place an LSTMCell and a GRUCell together inside this stack, and the RNN wrapper would run them as a sequence. This feature gives flexibility for experimentation, but it is not very commonly used in real applications.\n",
    "\n",
    "Finally, there are CuDNNGRU and CuDNNLSTM. These were special versions of GRU and LSTM designed to run much faster on NVIDIA GPUs by using the cuDNN library. They used to be important for performance, but in modern TensorFlow and Keras the standard GRU and LSTM layers automatically make use of GPU acceleration, so the CuDNN-specific classes are now mostly obsolete.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
