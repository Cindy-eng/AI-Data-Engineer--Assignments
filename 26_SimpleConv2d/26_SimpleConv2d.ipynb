{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6zFMv3y-hFEE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#【Problem 1】Creating a 2-D convolutional layer\n",
        "\n",
        "class Conv1d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, learning_rate=0.01):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        self.W = np.random.randn(out_channels, in_channels, kernel_size) * 0.01\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch_size, in_channels, in_width = x.shape\n",
        "        assert in_channels == self.in_channels\n",
        "\n",
        "        x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        out_width = (in_width + 2*self.padding - self.kernel_size) // self.stride + 1\n",
        "        out = np.zeros((batch_size, self.out_channels, out_width))\n",
        "\n",
        "        for n in range(batch_size):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_width):\n",
        "                    region = x_padded[n, :, i*self.stride:i*self.stride+self.kernel_size]\n",
        "                    out[n, m, i] = np.sum(region * self.W[m]) + self.b[m]\n",
        "\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        batch_size, _, out_width = dout.shape\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "        dx = np.zeros_like(self.x)\n",
        "\n",
        "        x_padded = np.pad(self.x, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
        "        dx_padded = np.pad(dx, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        for n in range(batch_size):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_width):\n",
        "                    region = x_padded[n, :, i*self.stride:i*self.stride+self.kernel_size]\n",
        "                    dW[m] += dout[n, m, i] * region\n",
        "                    db[m] += dout[n, m, i]\n",
        "                    dx_padded[n, :, i*self.stride:i*self.stride+self.kernel_size] += dout[n, m, i] * self.W[m]\n",
        "\n",
        "        if self.padding != 0:\n",
        "            dx = dx_padded[:, :, self.padding:-self.padding]\n",
        "        else:\n",
        "            dx = dx_padded\n",
        "\n",
        "        self.W -= self.lr * dW\n",
        "        self.b -= self.lr * db\n",
        "\n",
        "        return dx\n",
        "class Conv2d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, learning_rate=0.01):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        Fh, Fw = self.kernel_size\n",
        "        self.W = np.random.randn(out_channels, in_channels, Fh, Fw) * 0.01\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch_size, in_channels, in_h, in_w = x.shape\n",
        "        assert in_channels == self.in_channels\n",
        "\n",
        "        x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        Fh, Fw = self.kernel_size\n",
        "        out_h = (in_h + 2*self.padding - Fh) // self.stride + 1\n",
        "        out_w = (in_w + 2*self.padding - Fw) // self.stride + 1\n",
        "        out = np.zeros((batch_size, self.out_channels, out_h, out_w))\n",
        "\n",
        "        for n in range(batch_size):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        region = x_padded[n, :, i*self.stride:i*self.stride+Fh, j*self.stride:j*self.stride+Fw]\n",
        "                        out[n, m, i, j] = np.sum(region * self.W[m]) + self.b[m]\n",
        "\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        batch_size, _, out_h, out_w = dout.shape\n",
        "        Fh, Fw = self.kernel_size\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "        dx = np.zeros_like(self.x)\n",
        "\n",
        "        x_padded = np.pad(self.x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "        dx_padded = np.pad(dx, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        for n in range(batch_size):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        region = x_padded[n, :, i*self.stride:i*self.stride+Fh, j*self.stride:j*self.stride+Fw]\n",
        "                        dW[m] += dout[n, m, i, j] * region\n",
        "                        db[m] += dout[n, m, i, j]\n",
        "                        dx_padded[n, :, i*self.stride:i*self.stride+Fh, j*self.stride:j*self.stride+Fw] += dout[n, m, i, j] * self.W[m]\n",
        "\n",
        "        if self.padding != 0:\n",
        "            dx = dx_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        else:\n",
        "            dx = dx_padded\n",
        "\n",
        "        self.W -= self.lr * dW\n",
        "        self.b -= self.lr * db\n",
        "\n",
        "        return dx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 2] Experiments with 2D convolutional layers on small arrays\n",
        "x = np.array([[[[1, 2, 3, 4],\n",
        "                [5, 6, 7, 8],\n",
        "                [9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]], dtype=float)\n",
        "\n",
        "w = np.array([\n",
        "    [[0.0, 0.0, 0.0],\n",
        "     [0.0, 1.0, 0.0],\n",
        "     [0.0, -1.0, 0.0]],\n",
        "    [[0.0, 0.0, 0.0],\n",
        "     [0.0, -1.0, 1.0],\n",
        "     [0.0, 0.0, 0.0]]\n",
        "], dtype=float)\n",
        "\n",
        "b = np.zeros(2)\n",
        "\n",
        "def conv2d_forward(x, w, b):\n",
        "    batch, in_c, h, w_in = x.shape\n",
        "    out_c, k_h, k_w = w.shape\n",
        "    out_h = h - k_h + 1\n",
        "    out_w = w_in - k_w + 1\n",
        "    out = np.zeros((batch, out_c, out_h, out_w))\n",
        "    for n in range(batch):\n",
        "        for m in range(out_c):\n",
        "            for i in range(out_h):\n",
        "                for j in range(out_w):\n",
        "                    window = x[n, 0, i:i+k_h, j:j+k_w]\n",
        "                    out[n, m, i, j] = np.sum(window * w[m]) + b[m]\n",
        "    return out\n",
        "\n",
        "out = conv2d_forward(x, w, b)\n",
        "print(\"Forward output:\\n\", out[0])\n",
        "\n",
        "delta = np.array([[[-4, -4], [10, 11]], [[1, -7], [1, -11]]])\n",
        "\n",
        "def conv2d_backward_input(delta, w, x_shape):\n",
        "    batch, out_c, out_h, out_w = delta.shape\n",
        "    _, k_h, k_w = w.shape\n",
        "    dx = np.zeros(x_shape)\n",
        "    for n in range(batch):\n",
        "        for c in range(x_shape[1]):\n",
        "            for i in range(x_shape[2]):\n",
        "                for j in range(x_shape[3]):\n",
        "                    grad_sum = 0\n",
        "                    for m in range(out_c):\n",
        "                        for s in range(k_h):\n",
        "                            for t in range(k_w):\n",
        "                                i_out = i - s\n",
        "                                j_out = j - t\n",
        "                                if 0 <= i_out < out_h and 0 <= j_out < out_w:\n",
        "                                    grad_sum += delta[n, m, i_out, j_out] * w[m, s, t]\n",
        "                    dx[n, c, i, j] = grad_sum\n",
        "    return dx\n",
        "\n",
        "dx = conv2d_backward_input(delta.reshape(1, 2, 2, 2), w, x.shape)\n",
        "print(\"Backward dx:\\n\", dx[0,0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHElX7CCGp51",
        "outputId": "87e08e19-62ed-4450-9fb5-83c47bcbb6c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward output:\n",
            " [[[-4. -4.]\n",
            "  [-4. -4.]]\n",
            "\n",
            " [[ 1.  1.]\n",
            "  [ 1.  1.]]]\n",
            "Backward dx:\n",
            " [[  0.   0.   0.   0.]\n",
            " [  0.  -5.   4.  -7.]\n",
            " [  0.  13.  27. -11.]\n",
            " [  0. -10. -11.   0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 3] Output size after 2-dimensional convolution\n",
        "\n",
        "def conv2d_output_size(Nh_in, Nw_in, Fh, Fw, Ph=0, Pw=0, Sh=1, Sw=1):\n",
        "    Nh_out = (Nh_in + 2 * Ph - Fh) // Sh + 1\n",
        "    Nw_out = (Nw_in + 2 * Pw - Fw) // Sw + 1\n",
        "    return Nh_out, Nw_out\n"
      ],
      "metadata": {
        "id": "-KeHQBmBl8D8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 4] Creation of maximum pooling layer\n",
        "class MaxPool2D:\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch, channels, h, w = x.shape\n",
        "        Kh, Kw = self.kernel_size, self.kernel_size\n",
        "        Sh, Sw = self.stride, self.stride\n",
        "        Nh_out = (h - Kh) // Sh + 1\n",
        "        Nw_out = (w - Kw) // Sw + 1\n",
        "        out = np.zeros((batch, channels, Nh_out, Nw_out))\n",
        "        self.mask = np.zeros_like(x)\n",
        "\n",
        "        for n in range(batch):\n",
        "            for c in range(channels):\n",
        "                for i in range(Nh_out):\n",
        "                    for j in range(Nw_out):\n",
        "                        h_start = i * Sh\n",
        "                        w_start = j * Sw\n",
        "                        window = x[n, c, h_start:h_start+Kh, w_start:w_start+Kw]\n",
        "                        max_val = np.max(window)\n",
        "                        out[n, c, i, j] = max_val\n",
        "                        max_pos = (window == max_val)\n",
        "                        self.mask[n, c, h_start:h_start+Kh, w_start:w_start+Kw] += max_pos\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.zeros_like(self.x)\n",
        "        batch, channels, Nh_out, Nw_out = dout.shape\n",
        "        Kh, Kw = self.kernel_size, self.kernel_size\n",
        "        Sh, Sw = self.stride, self.stride\n",
        "\n",
        "        for n in range(batch):\n",
        "            for c in range(channels):\n",
        "                for i in range(Nh_out):\n",
        "                    for j in range(Nw_out):\n",
        "                        h_start = i * Sh\n",
        "                        w_start = j * Sw\n",
        "                        dx[n, c, h_start:h_start+Kh, w_start:w_start+Kw] += dout[n, c, i, j] * self.mask[n, c, h_start:h_start+Kh, w_start:w_start+Kw]\n",
        "        return dx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def GetMiniBatch(X, y, batch_size=1):\n",
        "    N = X.shape[0]\n",
        "    for i in range(0, N, batch_size):\n",
        "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
        "\n",
        "\n",
        "class SimpleConv2d:\n",
        "    def __init__(self, F, C, FH, FW, P=0, S=1):\n",
        "        self.F, self.C, self.FH, self.FW = F, C, FH, FW\n",
        "        self.P, self.S = P, S\n",
        "        limit = 1 / np.sqrt(C * FH * FW)\n",
        "        self.W = np.random.uniform(-limit, limit, (F, C, FH, FW))\n",
        "        self.b = np.zeros(F)\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = (H + 2*self.P - self.FH)//self.S + 1\n",
        "        out_w = (W + 2*self.P - self.FW)//self.S + 1\n",
        "        x_pad = np.pad(x, ((0,0),(0,0),(self.P,self.P),(self.P,self.P)), 'constant')\n",
        "        out = np.zeros((N, self.F, out_h, out_w))\n",
        "        for n in range(N):\n",
        "            for f in range(self.F):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i*self.S\n",
        "                        w_start = j*self.S\n",
        "                        window = x_pad[n, :, h_start:h_start+self.FH, w_start:w_start+self.FW]\n",
        "                        out[n,f,i,j] = np.sum(window * self.W[f]) + self.b[f]\n",
        "        self.out = out\n",
        "        return out\n",
        "    def backward(self, dout, lr=0.01):\n",
        "        N, C, H, W = self.x.shape\n",
        "        _, F, out_h, out_w = dout.shape\n",
        "        x_pad = np.pad(self.x, ((0,0),(0,0),(self.P,self.P),(self.P,self.P)), 'constant')\n",
        "        dx_pad = np.zeros_like(x_pad)\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "        for n in range(N):\n",
        "            for f in range(F):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i*self.S\n",
        "                        w_start = j*self.S\n",
        "                        window = x_pad[n, :, h_start:h_start+self.FH, w_start:w_start+self.FW]\n",
        "                        dW[f] += dout[n,f,i,j] * window\n",
        "                        db[f] += dout[n,f,i,j]\n",
        "                        dx_pad[n, :, h_start:h_start+self.FH, w_start:w_start+self.FW] += dout[n,f,i,j] * self.W[f]\n",
        "        if self.P > 0:\n",
        "            dx = dx_pad[:, :, self.P:-self.P, self.P:-self.P]\n",
        "        else:\n",
        "            dx = dx_pad\n",
        "        self.W -= lr * dW\n",
        "        self.b -= lr * db\n",
        "        return dx\n",
        "\n"
      ],
      "metadata": {
        "id": "n392ny-el_b-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 5] (Advance task) Creating average pooling\n",
        "\n",
        "class AveragePool2D:\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch, channels, h, w = x.shape\n",
        "        Kh, Kw = self.kernel_size, self.kernel_size\n",
        "        Sh, Sw = self.stride, self.stride\n",
        "        Nh_out = (h - Kh) // Sh + 1\n",
        "        Nw_out = (w - Kw) // Sw + 1\n",
        "        out = np.zeros((batch, channels, Nh_out, Nw_out))\n",
        "\n",
        "        for n in range(batch):\n",
        "            for c in range(channels):\n",
        "                for i in range(Nh_out):\n",
        "                    for j in range(Nw_out):\n",
        "                        h_start = i * Sh\n",
        "                        w_start = j * Sw\n",
        "                        window = x[n, c, h_start:h_start+Kh, w_start:w_start+Kw]\n",
        "                        out[n, c, i, j] = np.mean(window)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.zeros_like(self.x)\n",
        "        batch, channels, Nh_out, Nw_out = dout.shape\n",
        "        Kh, Kw = self.kernel_size, self.kernel_size\n",
        "        Sh, Sw = self.stride, self.stride\n",
        "\n",
        "        for n in range(batch):\n",
        "            for c in range(channels):\n",
        "                for i in range(Nh_out):\n",
        "                    for j in range(Nw_out):\n",
        "                        h_start = i * Sh\n",
        "                        w_start = j * Sw\n",
        "                        dx[n, c, h_start:h_start+Kh, w_start:w_start+Kw] += dout[n, c, i, j] / (Kh * Kw)\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "WwWYLJ9lmIZI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 6] Smoothing\n",
        "class Flatten:\n",
        "    def forward(self, x):\n",
        "        self.input_shape = x.shape\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout.reshape(self.input_shape)\n",
        "class FC:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        limit = 1/np.sqrt(input_dim)\n",
        "        self.W = np.random.uniform(-limit, limit, (input_dim, output_dim))\n",
        "        self.b = np.zeros(output_dim)\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return x @ self.W + self.b\n",
        "    def backward(self, dout, lr=0.01):\n",
        "        dW = self.x.T @ dout\n",
        "        db = np.sum(dout, axis=0)\n",
        "        dx = dout @ self.W.T\n",
        "        self.W -= lr * dW\n",
        "        self.b -= lr * db\n",
        "        return dx\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        self.mask = (x>0)\n",
        "        return x * self.mask\n",
        "    def backward(self, dout):\n",
        "        dx = dout.copy()\n",
        "        dx[self.mask] = 0\n",
        "        return dx\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        e = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        self.out = e / np.sum(e, axis=1, keepdims=True)\n",
        "        return self.out\n",
        "    def backward(self, dout):\n",
        "        return dout\n",
        "\n"
      ],
      "metadata": {
        "id": "SW5HIJzLmL1c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 7] Learning and estimation\n",
        "\n",
        "class Scratch2dCNNClassifier:\n",
        "    def __init__(self, NN, CNN, n_epoch=5, n_batch=20, verbose=False):\n",
        "        self.NN = NN\n",
        "        self.CNN = CNN\n",
        "        self.n_epoch = n_epoch\n",
        "        self.n_batch = n_batch\n",
        "        self.verbose = verbose\n",
        "        self.log_loss = np.zeros(n_epoch)\n",
        "        self.log_acc = np.zeros(n_epoch)\n",
        "        self.flt = Flatten()\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "    def loss_function(self, y, yt):\n",
        "        delta = 1e-7\n",
        "        return -np.mean(yt * np.log(y + delta))\n",
        "\n",
        "    def accuracy(self, Z, Y):\n",
        "        return accuracy_score(Y, Z)\n",
        "\n",
        "    def fit(self, X, y, X_val=False, y_val=False, lr=0.01):\n",
        "        for epoch in range(self.n_epoch):\n",
        "            get_mini_batch = list(GetMiniBatch(X, y, batch_size=self.n_batch))\n",
        "            self.loss = 0\n",
        "\n",
        "            for mini_X_train, mini_y_train in get_mini_batch:\n",
        "                forward_data = mini_X_train[:, np.newaxis, :, :]\n",
        "                for i in range(len(self.CNN)):\n",
        "                    forward_data = self.CNN[i].forward(forward_data)\n",
        "\n",
        "                flt = Flatten()\n",
        "                forward_data = flt.forward(forward_data)\n",
        "\n",
        "                for i in range(len(self.NN)):\n",
        "                    forward_data = self.NN[i].forward(forward_data)\n",
        "\n",
        "                Z = forward_data\n",
        "                dout = (Z - mini_y_train) / self.n_batch\n",
        "\n",
        "                for i in reversed(range(len(self.NN))):\n",
        "                    layer = self.NN[i]\n",
        "                    if isinstance(layer, FC) or isinstance(layer, SimpleConv2d):\n",
        "                        dout = layer.backward(dout, lr)\n",
        "                    else:\n",
        "                        dout = layer.backward(dout)\n",
        "\n",
        "                dout = flt.backward(dout)\n",
        "\n",
        "                for i in reversed(range(len(self.CNN))):\n",
        "                    layer = self.CNN[i]\n",
        "                    if isinstance(layer, FC) or isinstance(layer, SimpleConv2d):\n",
        "                        dout = layer.backward(dout, lr)\n",
        "                    else:\n",
        "                        dout = layer.backward(dout)\n",
        "\n",
        "                self.loss += self.loss_function(Z, mini_y_train)\n",
        "\n",
        "                if self.verbose:\n",
        "                    print()\n",
        "\n",
        "        if self.verbose:\n",
        "            pred_train = self.predict(X)\n",
        "            acc_train = self.accuracy(pred_train, np.argmax(y, axis=1))\n",
        "            print(f\"Epoch {epoch+1}/{self.n_epoch} - Loss: {self.loss / len(get_mini_batch):.6f} - Accuracy: {acc_train:.4f}\")\n",
        "\n",
        "        self.log_loss[epoch] = self.loss / len(get_mini_batch)\n",
        "        self.log_acc[epoch] = self.accuracy(self.predict(X), np.argmax(y, axis=1))\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred_data = X[:, np.newaxis, :, :]\n",
        "        for i in range(len(self.CNN)):\n",
        "            pred_data = self.CNN[i].forward(pred_data)\n",
        "        pred_data = self.flt.forward(pred_data)\n",
        "        for i in range(len(self.NN)):\n",
        "            pred_data = self.NN[i].forward(pred_data)\n",
        "        return np.argmax(pred_data, axis=1)"
      ],
      "metadata": {
        "id": "vStyxC3ZHQoT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0\n",
        "y_train_one_hot = to_categorical(y_train, 10)\n",
        "y_test_one_hot = to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "NN = {\n",
        "    0: FC(10*13*13, 200),\n",
        "    1: ReLU(),\n",
        "    2: FC(200, 200),\n",
        "    3: ReLU(),\n",
        "    4: FC(200, 10)\n",
        "}\n",
        "\n",
        "CNN = {\n",
        "    0: SimpleConv2d(F=10, C=1, FH=3, FW=3, P=0, S=1),\n",
        "    1: MaxPool2D(kernel_size=2, stride=2)\n",
        "}\n",
        "\n",
        "\n",
        "cnn1 = Scratch2dCNNClassifier(NN=NN, CNN=CNN, n_epoch=1, n_batch=64, verbose=True)\n",
        "cnn1.fit(x_train[:1000], y_train_one_hot[:1000], lr=0.01)\n",
        "\n",
        "y_pred = cnn1.predict(x_test[:200])\n",
        "acc = accuracy_score(np.argmax(y_test_one_hot[:200], axis=1), y_pred)\n",
        "print(f\"Accuracy on test set (1000 samples): {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o6yuw0WJaJh",
        "outputId": "579cb175-47d5-4f9c-bb4a-796f24db14df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2724602960.py:17: RuntimeWarning: invalid value encountered in log\n",
            "  return -np.mean(yt * np.log(y + delta))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/1 - Loss: nan - Accuracy: 0.1230\n",
            "Accuracy on test set (1000 samples): 0.0800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 8] (Advance assignment) LeNet\n",
        "class ScratchLeNet:\n",
        "    def __init__(self):\n",
        "        # CNN layers\n",
        "        self.CNN = {\n",
        "            0: SimpleConv2d(F=6, C=1, FH=5, FW=5, P=0, S=1),  # 28x28 -> 24x24\n",
        "            1: ReLU(),\n",
        "            2: MaxPool2D(kernel_size=2, stride=2),             # 24x24 -> 12x12\n",
        "            3: SimpleConv2d(F=16, C=6, FH=5, FW=5, P=0, S=1), # 12x12 -> 8x8\n",
        "            4: ReLU(),\n",
        "            5: MaxPool2D(kernel_size=2, stride=2),             # 8x8 -> 4x4\n",
        "        }\n",
        "        self.flatten = Flatten()\n",
        "        # Fully connected layers\n",
        "        self.NN = {\n",
        "            0: FC(16*4*4, 120),\n",
        "            1: ReLU(),\n",
        "            2: FC(120, 84),\n",
        "            3: ReLU(),\n",
        "            4: FC(84, 10),\n",
        "            5: Softmax()\n",
        "        }\n",
        "\n",
        "        self.n_epoch = 5\n",
        "        self.n_batch = 64\n",
        "        self.log_loss = np.zeros(self.n_epoch)\n",
        "        self.log_acc = np.zeros(self.n_epoch)\n",
        "\n",
        "    def loss_function(self, y, yt):\n",
        "        delta = 1e-7\n",
        "        return -np.mean(yt * np.log(y + delta))\n",
        "\n",
        "    def accuracy(self, Z, Y):\n",
        "        return accuracy_score(Y, Z)\n",
        "\n",
        "    def fit(self, X, y, lr=0.01):\n",
        "        for epoch in range(self.n_epoch):\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
        "            loss_total = 0\n",
        "            for mini_X, mini_y in get_mini_batch:\n",
        "                # Forward pass CNN\n",
        "                forward_data = mini_X[:, np.newaxis, :, :]  # add channel dim\n",
        "                for i in range(len(self.CNN)):\n",
        "                    forward_data = self.CNN[i].forward(forward_data)\n",
        "\n",
        "                # Flatten\n",
        "                forward_data = self.flatten.forward(forward_data)\n",
        "\n",
        "                # Forward pass FC\n",
        "                for i in range(len(self.NN)):\n",
        "                    forward_data = self.NN[i].forward(forward_data)\n",
        "\n",
        "                Z = forward_data\n",
        "\n",
        "                # Backward\n",
        "                dout = (Z - mini_y) / self.n_batch\n",
        "\n",
        "                for i in reversed(range(len(self.NN))):\n",
        "                    layer = self.NN[i]\n",
        "                    if isinstance(layer, FC):\n",
        "                        dout = layer.backward(dout, lr)\n",
        "                    else:\n",
        "                        dout = layer.backward(dout)\n",
        "\n",
        "                dout = self.flatten.backward(dout)\n",
        "\n",
        "                for i in reversed(range(len(self.CNN))):\n",
        "                    layer = self.CNN[i]\n",
        "                    if isinstance(layer, SimpleConv2d):\n",
        "                        dout = layer.backward(dout, lr)\n",
        "                    else:\n",
        "                        dout = layer.backward(dout)\n",
        "\n",
        "                loss_total += self.loss_function(Z, mini_y)\n",
        "\n",
        "            self.log_loss[epoch] = loss_total / len(get_mini_batch)\n",
        "            pred = self.predict(X)\n",
        "            self.log_acc[epoch] = self.accuracy(pred, np.argmax(y, axis=1))\n",
        "            print(f\"Epoch {epoch+1}: Loss {self.log_loss[epoch]:.4f}, Accuracy {self.log_acc[epoch]:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        forward_data = X[:, np.newaxis, :, :]\n",
        "        for i in range(len(self.CNN)):\n",
        "            forward_data = self.CNN[i].forward(forward_data)\n",
        "        forward_data = self.flatten.forward(forward_data)\n",
        "        for i in range(len(self.NN)):\n",
        "            forward_data = self.NN[i].forward(forward_data)\n",
        "        return np.argmax(forward_data, axis=1)\n"
      ],
      "metadata": {
        "id": "ILHOEhdwOz4B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problem 9] (Advance assignment) Survey of famous image recognition models\n",
        "###AlexNet (2012)\n",
        "\n",
        "AlexNet was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton and is often credited with launching the deep learning revolution in computer vision. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 by a significant margin. The architecture consists of 8 layers, including 5 convolutional layers followed by 3 fully connected layers. It introduced the use of ReLU activation functions, which helped speed up training compared to previous sigmoid or tanh activations. AlexNet uses max-pooling layers for downsampling and incorporates dropout in the fully connected layers to reduce overfitting. The model also employed data augmentation and image preprocessing techniques to improve generalization. Because of its size and computational demands, it was trained on two GPUs. AlexNet demonstrated the effectiveness of deep convolutional neural networks trained on large datasets with GPUs, setting a new standard for image classification models.\n",
        "\n",
        "###VGG16 (2014)\n",
        "\n",
        "VGG16 was created by Karen Simonyan and Andrew Zisserman from the Visual Geometry Group at Oxford University. It achieved second place in the ILSVRC 2014 competition but is known for its simple and uniform architecture that emphasizes the importance of network depth. The model is 16 layers deep, composed of 13 convolutional layers and 3 fully connected layers. VGG16 uses very small 3x3 convolutional filters with stride 1 and padding 1. Multiple convolutional layers are stacked before each max-pooling layer, which increases the network’s expressiveness and non-linearity. The model uses ReLU activations throughout and ends with fully connected layers for classification. VGG16 demonstrated that very deep networks with small filters could significantly improve performance. Its simple and modular design makes it easy to implement and modify, and it is commonly used as a baseline or feature extractor in transfer learning.\n",
        "\n",
        "###Other Notable CNN Architectures (brief mentions)\n",
        "LeNet (1998): One of the earliest CNNs, designed for digit recognition (MNIST). Simpler and shallower than later models.\n",
        "\n",
        "GoogLeNet / Inception (2014): Introduced the Inception module with multiple filter sizes in parallel, deeper and more computationally efficient.\n",
        "\n",
        "ResNet (2015): Introduced residual connections to train very deep networks (50, 101, 152 layers) by addressing vanishing gradients.\n",
        "\n",
        "DenseNet, MobileNet, EfficientNet: More recent architectures focusing on parameter efficiency, speed, and accuracy trade-offs."
      ],
      "metadata": {
        "id": "8MIOgvVYPEPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 10] Calculation of output size and number of parameters\n",
        "def conv2d_output_size(H_in, W_in, Fh, Fw, stride=1, pad=0):\n",
        "    H_out = (H_in - Fh + 2*pad)//stride + 1\n",
        "    W_out = (W_in - Fw + 2*pad)//stride + 1\n",
        "    return H_out, W_out\n",
        "\n",
        "def conv2d_params(Fh, Fw, C_in, C_out, bias=True):\n",
        "    params = Fh * Fw * C_in * C_out\n",
        "    if bias:\n",
        "        params += C_out\n",
        "    return params\n",
        "\n",
        "layers = [\n",
        "    {\"H_in\":144, \"W_in\":144, \"C_in\":3, \"Fh\":3, \"Fw\":3, \"C_out\":6, \"stride\":1, \"pad\":0},\n",
        "    {\"H_in\":60,  \"W_in\":60,  \"C_in\":24,\"Fh\":3, \"Fw\":3, \"C_out\":48,\"stride\":1, \"pad\":0},\n",
        "    {\"H_in\":20,  \"W_in\":20,  \"C_in\":10,\"Fh\":3, \"Fw\":3, \"C_out\":20,\"stride\":2, \"pad\":0}\n",
        "]\n",
        "\n",
        "for i, l in enumerate(layers, 1):\n",
        "    H_out, W_out = conv2d_output_size(l[\"H_in\"], l[\"W_in\"], l[\"Fh\"], l[\"Fw\"], l[\"stride\"], l[\"pad\"])\n",
        "    params = conv2d_params(l[\"Fh\"], l[\"Fw\"], l[\"C_in\"], l[\"C_out\"])\n",
        "    print(f\"Layer {i}: Output size = {H_out}x{W_out}x{l['C_out']}, Params = {params}\")\n"
      ],
      "metadata": {
        "id": "bCBFKM7fMAW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac22f4b9-6d80-4f14-fa34-bceab86e904c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: Output size = 142x142x6, Params = 168\n",
            "Layer 2: Output size = 58x58x48, Params = 10416\n",
            "Layer 3: Output size = 9x9x20, Params = 1820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problem 11] (Advance assignment) Survey on filter size\n",
        "####3×3 convolution filters\n",
        "\n",
        "Preferred over fully connected networks because they reduce computational costs and allow weight sharing, resulting in fewer weights for backpropagation.\n",
        "\n",
        "Can be seen as a series of one-dimensional convolutions, making them cost-efficient.\n",
        "\n",
        "Work well in general and are often the popular choice for convolution layers.\n",
        "\n",
        "####1×1 convolution filters\n",
        "\n",
        "Proposed in the Network-in-Network paper and widely used in Google Inception.\n",
        "\n",
        "Advantages include:\n",
        "\n",
        "Dimensionality reduction for efficient computation\n",
        "\n",
        "Low-dimensional embedding or feature pooling\n",
        "\n",
        "Adding nonlinearity again after convolution"
      ],
      "metadata": {
        "id": "4Tg48uXoQE0N"
      }
    }
  ]
}