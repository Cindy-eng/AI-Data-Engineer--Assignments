{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6zFMv3y-hFEE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#【Problem 1】Creating a 2-D convolutional layer\n",
        "\n",
        "class Conv1d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, learning_rate=0.01):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        # Pesos e bias\n",
        "        self.W = np.random.randn(out_channels, in_channels, kernel_size) * 0.01\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch_size, in_channels, in_width = x.shape\n",
        "        assert in_channels == self.in_channels\n",
        "\n",
        "        # Padding\n",
        "        x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        # Tamanho da saída\n",
        "        out_width = (in_width + 2*self.padding - self.kernel_size) // self.stride + 1\n",
        "        out = np.zeros((batch_size, self.out_channels, out_width))\n",
        "\n",
        "        # Convolução\n",
        "        for n in range(batch_size):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_width):\n",
        "                    region = x_padded[n, :, i*self.stride:i*self.stride+self.kernel_size]\n",
        "                    out[n, m, i] = np.sum(region * self.W[m]) + self.b[m]\n",
        "\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        batch_size, _, out_width = dout.shape\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "        dx = np.zeros_like(self.x)\n",
        "\n",
        "        x_padded = np.pad(self.x, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
        "        dx_padded = np.pad(dx, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        # Gradientes\n",
        "        for n in range(batch_size):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_width):\n",
        "                    region = x_padded[n, :, i*self.stride:i*self.stride+self.kernel_size]\n",
        "                    dW[m] += dout[n, m, i] * region\n",
        "                    db[m] += dout[n, m, i]\n",
        "                    dx_padded[n, :, i*self.stride:i*self.stride+self.kernel_size] += dout[n, m, i] * self.W[m]\n",
        "\n",
        "        # Remover padding\n",
        "        if self.padding != 0:\n",
        "            dx = dx_padded[:, :, self.padding:-self.padding]\n",
        "        else:\n",
        "            dx = dx_padded\n",
        "\n",
        "        # Atualização dos parâmetros\n",
        "        self.W -= self.lr * dW\n",
        "        self.b -= self.lr * db\n",
        "\n",
        "        return dx\n",
        "class Conv2d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, learning_rate=0.01):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        Fh, Fw = self.kernel_size\n",
        "        self.W = np.random.randn(out_channels, in_channels, Fh, Fw) * 0.01\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch_size, in_channels, in_h, in_w = x.shape\n",
        "        assert in_channels == self.in_channels\n",
        "\n",
        "        # Padding\n",
        "        x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        Fh, Fw = self.kernel_size\n",
        "        out_h = (in_h + 2*self.padding - Fh) // self.stride + 1\n",
        "        out_w = (in_w + 2*self.padding - Fw) // self.stride + 1\n",
        "        out = np.zeros((batch_size, self.out_channels, out_h, out_w))\n",
        "\n",
        "        # Convolução\n",
        "        for n in range(batch_size):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        region = x_padded[n, :, i*self.stride:i*self.stride+Fh, j*self.stride:j*self.stride+Fw]\n",
        "                        out[n, m, i, j] = np.sum(region * self.W[m]) + self.b[m]\n",
        "\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        batch_size, _, out_h, out_w = dout.shape\n",
        "        Fh, Fw = self.kernel_size\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "        dx = np.zeros_like(self.x)\n",
        "\n",
        "        x_padded = np.pad(self.x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "        dx_padded = np.pad(dx, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        for n in range(batch_size):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        region = x_padded[n, :, i*self.stride:i*self.stride+Fh, j*self.stride:j*self.stride+Fw]\n",
        "                        dW[m] += dout[n, m, i, j] * region\n",
        "                        db[m] += dout[n, m, i, j]\n",
        "                        dx_padded[n, :, i*self.stride:i*self.stride+Fh, j*self.stride:j*self.stride+Fw] += dout[n, m, i, j] * self.W[m]\n",
        "\n",
        "        if self.padding != 0:\n",
        "            dx = dx_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        else:\n",
        "            dx = dx_padded\n",
        "\n",
        "        self.W -= self.lr * dW\n",
        "        self.b -= self.lr * db\n",
        "\n",
        "        return dx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 2] Experiments with 2D convolutional layers on small arrays\n",
        "x = np.array([[[[1, 2, 3, 4],\n",
        "                [5, 6, 7, 8],\n",
        "                [9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]], dtype=float)\n",
        "\n",
        "w = np.array([\n",
        "    [[0.0, 0.0, 0.0],\n",
        "     [0.0, 1.0, 0.0],\n",
        "     [0.0, -1.0, 0.0]],\n",
        "    [[0.0, 0.0, 0.0],\n",
        "     [0.0, -1.0, 1.0],\n",
        "     [0.0, 0.0, 0.0]]\n",
        "], dtype=float)\n",
        "\n",
        "b = np.zeros(2)\n",
        "\n",
        "def conv2d_forward(x, w, b):\n",
        "    batch, in_c, h, w_in = x.shape\n",
        "    out_c, k_h, k_w = w.shape\n",
        "    out_h = h - k_h + 1\n",
        "    out_w = w_in - k_w + 1\n",
        "    out = np.zeros((batch, out_c, out_h, out_w))\n",
        "    for n in range(batch):\n",
        "        for m in range(out_c):\n",
        "            for i in range(out_h):\n",
        "                for j in range(out_w):\n",
        "                    window = x[n, 0, i:i+k_h, j:j+k_w]\n",
        "                    out[n, m, i, j] = np.sum(window * w[m]) + b[m]\n",
        "    return out\n",
        "\n",
        "out = conv2d_forward(x, w, b)\n",
        "print(\"Forward output:\\n\", out[0])\n",
        "\n",
        "delta = np.array([[[-4, -4], [10, 11]], [[1, -7], [1, -11]]])\n",
        "\n",
        "def conv2d_backward_input(delta, w, x_shape):\n",
        "    batch, out_c, out_h, out_w = delta.shape\n",
        "    _, k_h, k_w = w.shape\n",
        "    dx = np.zeros(x_shape)\n",
        "    for n in range(batch):\n",
        "        for c in range(x_shape[1]):\n",
        "            for i in range(x_shape[2]):\n",
        "                for j in range(x_shape[3]):\n",
        "                    grad_sum = 0\n",
        "                    for m in range(out_c):\n",
        "                        for s in range(k_h):\n",
        "                            for t in range(k_w):\n",
        "                                i_out = i - s\n",
        "                                j_out = j - t\n",
        "                                if 0 <= i_out < out_h and 0 <= j_out < out_w:\n",
        "                                    grad_sum += delta[n, m, i_out, j_out] * w[m, s, t]\n",
        "                    dx[n, c, i, j] = grad_sum\n",
        "    return dx\n",
        "\n",
        "dx = conv2d_backward_input(delta.reshape(1, 2, 2, 2), w, x.shape)\n",
        "print(\"Backward dx:\\n\", dx[0,0])\n",
        "\n"
      ],
      "metadata": {
        "id": "LHElX7CCGp51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 3] Output size after 2-dimensional convolution\n",
        "\n",
        "def conv2d_output_size(Nh_in, Nw_in, Fh, Fw, Ph=0, Pw=0, Sh=1, Sw=1):\n",
        "    Nh_out = (Nh_in + 2 * Ph - Fh) // Sh + 1\n",
        "    Nw_out = (Nw_in + 2 * Pw - Fw) // Sw + 1\n",
        "    return Nh_out, Nw_out\n"
      ],
      "metadata": {
        "id": "-KeHQBmBl8D8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 4] Creation of maximum pooling layer\n",
        "class MaxPool2D:\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch, channels, h, w = x.shape\n",
        "        Kh, Kw = self.kernel_size, self.kernel_size\n",
        "        Sh, Sw = self.stride, self.stride\n",
        "        Nh_out = (h - Kh) // Sh + 1\n",
        "        Nw_out = (w - Kw) // Sw + 1\n",
        "        out = np.zeros((batch, channels, Nh_out, Nw_out))\n",
        "        self.mask = np.zeros_like(x)\n",
        "\n",
        "        for n in range(batch):\n",
        "            for c in range(channels):\n",
        "                for i in range(Nh_out):\n",
        "                    for j in range(Nw_out):\n",
        "                        h_start = i * Sh\n",
        "                        w_start = j * Sw\n",
        "                        window = x[n, c, h_start:h_start+Kh, w_start:w_start+Kw]\n",
        "                        max_val = np.max(window)\n",
        "                        out[n, c, i, j] = max_val\n",
        "                        # Mask: marcar a posição do máximo para backward\n",
        "                        max_pos = (window == max_val)\n",
        "                        self.mask[n, c, h_start:h_start+Kh, w_start:w_start+Kw] += max_pos\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.zeros_like(self.x)\n",
        "        batch, channels, Nh_out, Nw_out = dout.shape\n",
        "        Kh, Kw = self.kernel_size, self.kernel_size\n",
        "        Sh, Sw = self.stride, self.stride\n",
        "\n",
        "        for n in range(batch):\n",
        "            for c in range(channels):\n",
        "                for i in range(Nh_out):\n",
        "                    for j in range(Nw_out):\n",
        "                        h_start = i * Sh\n",
        "                        w_start = j * Sw\n",
        "                        dx[n, c, h_start:h_start+Kh, w_start:w_start+Kw] += dout[n, c, i, j] * self.mask[n, c, h_start:h_start+Kh, w_start:w_start+Kw]\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "n392ny-el_b-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 5] (Advance task) Creating average pooling\n",
        "\n",
        "class AveragePool2D:\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch, channels, h, w = x.shape\n",
        "        Kh, Kw = self.kernel_size, self.kernel_size\n",
        "        Sh, Sw = self.stride, self.stride\n",
        "        Nh_out = (h - Kh) // Sh + 1\n",
        "        Nw_out = (w - Kw) // Sw + 1\n",
        "        out = np.zeros((batch, channels, Nh_out, Nw_out))\n",
        "\n",
        "        for n in range(batch):\n",
        "            for c in range(channels):\n",
        "                for i in range(Nh_out):\n",
        "                    for j in range(Nw_out):\n",
        "                        h_start = i * Sh\n",
        "                        w_start = j * Sw\n",
        "                        window = x[n, c, h_start:h_start+Kh, w_start:w_start+Kw]\n",
        "                        out[n, c, i, j] = np.mean(window)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.zeros_like(self.x)\n",
        "        batch, channels, Nh_out, Nw_out = dout.shape\n",
        "        Kh, Kw = self.kernel_size, self.kernel_size\n",
        "        Sh, Sw = self.stride, self.stride\n",
        "\n",
        "        for n in range(batch):\n",
        "            for c in range(channels):\n",
        "                for i in range(Nh_out):\n",
        "                    for j in range(Nw_out):\n",
        "                        h_start = i * Sh\n",
        "                        w_start = j * Sw\n",
        "                        dx[n, c, h_start:h_start+Kh, w_start:w_start+Kw] += dout[n, c, i, j] / (Kh * Kw)\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "WwWYLJ9lmIZI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Problem 6] Smoothing\n",
        "class Flatten:\n",
        "    def forward(self, x):\n",
        "        self.input_shape = x.shape\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout.reshape(self.input_shape)\n"
      ],
      "metadata": {
        "id": "SW5HIJzLmL1c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "class Scratch2dCNNClassifier:\n",
        "    def __init__(self, NN, CNN, n_epoch=5, n_batch=20, verbose=False):\n",
        "        self.NN = NN  # dict de layers NN (FC etc)\n",
        "        self.CNN = CNN  # dict de layers CNN (Conv, Pool)\n",
        "        self.n_epoch = n_epoch\n",
        "        self.n_batch = n_batch\n",
        "        self.verbose = verbose\n",
        "        self.log_loss = np.zeros(n_epoch)\n",
        "        self.log_acc = np.zeros(n_epoch)\n",
        "        self.flt = Flatten()\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "    def loss_function(self, y, yt):\n",
        "        delta = 1e-7\n",
        "        return -np.mean(yt * np.log(y + delta))\n",
        "\n",
        "    def accuracy(self, Z, Y):\n",
        "        return accuracy_score(Y, Z)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None, lr=0.01):\n",
        "        N = X.shape[0]\n",
        "        for epoch in range(self.n_epoch):\n",
        "            batch_loss = 0\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
        "            for mini_X, mini_y in get_mini_batch:\n",
        "                mini_X = mini_X[:, np.newaxis, :, :]  # adicionar canal\n",
        "                # Forward CNN\n",
        "                forward_data = mini_X\n",
        "                for i in range(len(self.CNN)):\n",
        "                    forward_data = self.CNN[i].forward(forward_data)\n",
        "                # Flatten\n",
        "                forward_data = self.flt.forward(forward_data)\n",
        "                # Forward NN\n",
        "                for i in range(len(self.NN)):\n",
        "                    forward_data = self.NN[i].forward(forward_data)\n",
        "                # Softmax\n",
        "                out = self.softmax.forward(forward_data)\n",
        "\n",
        "                # Backprop cross entropy gradient\n",
        "                dout = (out - mini_y) / self.n_batch\n",
        "\n",
        "                # Backward NN\n",
        "                for i in reversed(range(len(self.NN))):\n",
        "                    dout = self.NN[i].backward(dout, lr)\n",
        "\n",
        "                # Backward Flatten\n",
        "                dout = self.flt.backward(dout)\n",
        "\n",
        "                # Backward CNN\n",
        "                for i in reversed(range(len(self.CNN))):\n",
        "                    dout = self.CNN[i].backward(dout, lr)\n",
        "\n",
        "                batch_loss += self.loss_function(out, mini_y)\n",
        "\n",
        "            self.log_loss[epoch] = batch_loss / (N / self.n_batch)\n",
        "            y_pred = self.predict(X)\n",
        "            self.log_acc[epoch] = self.accuracy(y_pred, np.argmax(y, axis=1))\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"Epoch {epoch+1}/{self.n_epoch} - Loss: {self.log_loss[epoch]:.4f} - Acc: {self.log_acc[epoch]:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred_data = X[:, np.newaxis, :, :]\n",
        "        for i in range(len(self.CNN)):\n",
        "            pred_data = self.CNN[i].forward(pred_data)\n",
        "        pred_data = self.flt.forward(pred_data)\n",
        "        for i in range(len(self.NN)):\n",
        "            pred_data = self.NN[i].forward(pred_data)\n",
        "        return np.argmax(pred_data, axis=1)"
      ],
      "metadata": {
        "id": "vStyxC3ZHQoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}